{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3922516,"sourceType":"datasetVersion","datasetId":2329262}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install gdown","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AdamW,\n    PegasusForConditionalGeneration,\n    PegasusTokenizerFast as PegasusTokenizer\n)\nfrom tqdm.auto import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token = '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'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\nfrom pytorch_lightning.loggers import TensorBoardLogger","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport numpy as np\nimport torch\nimport transformers\nfrom pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader\n\n#from pytorch_lightning.callbacks import ModelCheckpoint\n#from pytorch_lightning.loggers import TensorBoardLogger\nfrom sklearn.model_selection import train_test_split\nfrom termcolor import colored\nimport textwrap","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !gdown 15WyzzCmFGXQjQuSzABIxaEjXWmE1aDJv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/Dataset_articles_NoID-2.csv', encoding = \"utf-8\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[[\"Summary\", \"Contents\"]]\ndf.columns = [\"summary\", \"text\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.dropna()\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nsns.set(style='whitegrid', palette='muted', font_scale = 1.2)\nrcParams['figure.figsize'] = 16, 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pl.seed_everything(42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, test_df = train_test_split(df, test_size = 0.0001)\ntrain_df.shape, test_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataSet(Dataset):\n    def __init__(\n            self,\n            data: pd.DataFrame,\n            tokenizer: PegasusTokenizer,\n            text_max_token_len: int = 512,\n            summary_max_token_len: int = 300,\n    ):\n        self.tokenizer = tokenizer\n        self.data = data\n        self.text_max_token_len = text_max_token_len\n        self.summary_max_token_len = summary_max_token_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index: int):\n        data_row = self.data.iloc[index]\n\n        text = data_row[\"text\"]\n        text_encoding = tokenizer(\n            text,\n            max_length=self.text_max_token_len,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True,\n            add_special_tokens=True,\n            return_tensors=\"pt\"\n        )\n        summary_encoding = tokenizer(\n            data_row[\"summary\"],\n            max_length=self.summary_max_token_len,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True,\n            add_special_tokens=True,\n            return_tensors=\"pt\"\n        )\n        labels = summary_encoding[\"input_ids\"]\n        labels[labels == 0] = -100\n\n        return dict(\n            text=text,\n            summary=data_row[\"summary\"],\n            text_input_ids=text_encoding[\"input_ids\"].flatten(),\n            text_attention_mask=text_encoding[\"attention_mask\"].flatten(),\n            labels=labels.flatten(),\n            labels_attention_mask=summary_encoding[\"attention_mask\"].flatten()\n        )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SummaryDataModule(pl.LightningDataModule):\n    def __init__(\n            self,\n            train_df: pd.DataFrame,\n            test_df: pd.DataFrame,\n            tokenizer: PegasusTokenizer,\n            batch_size: int = 8,\n            text_max_token_len: int = 512,\n            summary_max_token_len: int = 300\n    ):\n        super().__init__()\n        self.train_df = train_df\n        self.test_df = test_df\n\n        self.batch_size = batch_size\n        self.tokenizer = tokenizer\n\n        self.text_max_token_len = text_max_token_len\n        self.summary_max_token_len = summary_max_token_len\n\n    def setup(self, stage=None):\n        self.train_dataset = DataSet(\n            self.train_df,\n            self.tokenizer,\n            self.text_max_token_len,\n            self.summary_max_token_len\n        )\n        self.test_dataset = DataSet(\n            self.test_df,\n            self.tokenizer,\n            self.text_max_token_len,\n            self.summary_max_token_len\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=2\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=2\n        )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL_NAME = 'google/bigbird-pegasus-large-arxiv'\nMODEL_NAME = 'google/pegasus-cnn_dailymail'\ntokenizer = PegasusTokenizer.from_pretrained(MODEL_NAME, token=token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_token_counts, summary_token_counts = [], []\n\nfor _, row in tqdm(train_df.iterrows()):\n#     bigbird-pegasus-large-arxiv\n#     text_token_count = len(tokenizer.encode(row[\"text\"], max_length=4096, truncation=True))\n#     google/pegasus-cnn_dailymail\n    text_token_count = len(tokenizer.encode(row[\"text\"], max_length=1024, truncation=True))\n    text_token_counts.append(text_token_count)\n    summary_token_count = len(tokenizer.encode(row[\"summary\"]))\n    summary_token_counts.append(summary_token_count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2)\n\nsns.histplot(text_token_counts, ax = ax1)\nax1.set_title(\"full text token counts\")\n\nsns.histplot(summary_token_counts, ax= ax2)\nax2.set_title(\"summary text token counts\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 4\nBATCH_SIZE = 2\n\ndata_module = SummaryDataModule(train_df, test_df, tokenizer,batch_size = BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SummaryModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = PegasusForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True, token=token)\n\n    def forward(self, input_ids, attention_mask, decoder_attention_mask, labels=None):\n        output = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            decoder_attention_mask=decoder_attention_mask\n        )\n\n        return output.loss, output.logits\n\n    def training_step(self, batch, batch_idx):\n        input_ids = batch[\"text_input_ids\"]\n        attention_mask = batch[\"text_attention_mask\"]\n        labels = batch[\"labels\"]\n        labels_attention_mask = batch[\"labels_attention_mask\"]\n\n        loss, outputs = self(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            decoder_attention_mask=labels_attention_mask,\n            labels=labels\n        )\n        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        input_ids = batch[\"text_input_ids\"]\n        attention_mask = batch[\"text_attention_mask\"]\n        labels = batch[\"labels\"]\n        labels_attention_mask = batch[\"labels_attention_mask\"]\n\n        loss, outputs = self(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            decoder_attention_mask=labels_attention_mask,\n            labels=labels\n        )\n        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        input_ids = batch[\"text_input_ids\"]\n        attention_mask = batch[\"text_attention_mask\"]\n        labels = batch[\"labels\"]\n        labels_attention_mask = batch[\"labels_attention_mask\"]\n\n        loss, outputs = self(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            decoder_attention_mask=labels_attention_mask,\n            labels=labels\n        )\n\n        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n        return loss\n\n    def configure_optimizers(self):\n        return AdamW(self.parameters(), lr=5e-4)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SummaryModel()\n# model = SummaryModel.load_from_checkpoint('best.ckpt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard \n%tensorboard --logdir ./linghtning_logs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_callback = ModelCheckpoint(\n        dirpath =\"/kaggle/working/checkpoints\",\n        filename = \"best-checkpoint\",\n        save_top_k=1,\n        verbose=True,\n        monitor='val_loss',\n        mode='min'\n    )\nlogger = TensorBoardLogger(\"lightning_logs\", name = \"news-summary\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tạo callback cho thanh tiến trình\nprogress_bar = TQDMProgressBar(refresh_rate=1)  # Cập nhật mỗi 10 giây","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = pl.Trainer(\n    logger = logger, \n    callbacks=[checkpoint_callback, progress_bar],  # Thay đổi này \n    max_epochs = num_epochs, \n    accelerator=\"gpu\",\n    devices=-1\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.fit(model, data_module)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model = SummaryModel.load_from_checkpoint( \n    trainer.checkpoint_callback.best_model_path\n)\n\ntrained_model.freeze()\ntrained_model.to('cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def summarize(text):\n    # Encode văn bản đầu vào với mã hóa UTF-8\n    text = text.encode('utf-8').decode('utf-8')\n    \n    text_encoding = tokenizer(\n        text,\n        max_length=512,\n        padding=\"max_length\",\n        truncation=True,\n        return_attention_mask=True,\n        add_special_tokens=True,\n        return_tensors=\"pt\"\n    )\n\n    generated_ids = trained_model.model.generate(\n        input_ids=text_encoding[\"input_ids\"],\n        attention_mask=text_encoding[\"attention_mask\"],\n        max_length=300,\n        num_beams=2,\n        repetition_penalty=2.5,\n        length_penalty=1.0,\n        early_stopping=True\n    )\n\n    preds = [\n        tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        for gen_id in generated_ids\n    ]\n\n    return \"\".join(preds)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_row = test_df.iloc[0]\ntext = sample_row[\"text\"]\nmodel_summary = summarize(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_row[\"summary\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"\"\"\nSpeech processing is the study of speech signals and the processing methods of signals. The signals are usually processed in a digital representation, so speech processing can be regarded as a special case of digital signal processing, applied to speech signals. Aspects of speech processing includes the acquisition, manipulation, storage, transfer and output of speech signals. Different speech processing tasks include speech recognition, speech synthesis, speaker diarization, speech enhancement, speaker recognition, etc.[1]\n\"\"\"\nresult = summarize(text)\nresult","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch-summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary\nsummary(trained_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}